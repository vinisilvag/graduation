{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df6d0ed",
   "metadata": {},
   "source": [
    "# TP2 - Realidade Aumentada\n",
    "\n",
    "__Aluno:__ Vinicius Silva Gomes\n",
    "\n",
    "__Matrícula:__ 2021421869\n",
    "\n",
    "__Link do vídeo:__ [https://www.youtube.com/watch?v=cS0NltWVnqo&ab_channel=ViniciusGomes](https://www.youtube.com/watch?v=cS0NltWVnqo&ab_channel=ViniciusGomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1145329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.10.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "NumPy: 1.23.2\n",
      "OpenCV: 4.6.0\n",
      "OpenGL: 3.1.6\n",
      "PIL: 9.0.1\n",
      "Python: 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]\n",
      "PyGame: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas necessárias para o programa\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import PIL\n",
    "import pygame as pg\n",
    "\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLUT import *\n",
    "from OpenGL.GLU import *\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from objloader import *\n",
    "\n",
    "print('NumPy:', np.__version__)\n",
    "print('OpenCV:', cv2.__version__)\n",
    "print('OpenGL:', OpenGL.__version__)\n",
    "print('PIL:', PIL.__version__)\n",
    "print('Python:', sys.version)\n",
    "print('PyGame:', pg.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258554c8",
   "metadata": {},
   "source": [
    "## Extraindo frames do vídeo para calibração\n",
    "\n",
    "Esse pequeno script foi feito para extrair os frames do vídeo e usá-los para calibrar a câmera e obter os parâmetros intrínsecos da câmera. O script, em suma, carrega o vídeo, usando o OpenCV, e salva cada frame no disco, em uma pasta chamada ./fames. O script facilitou a extração dos frames e tornou o processo mais automático, por isso ele foi utilizado.\n",
    "\n",
    "Dessa pasta, foram escolhidos 5 frames que apresentavam angulações, distâncias e rotações diferentes do tabuleiro xadrez e esses frames foram separados para serem usados na calibração da câmera. Esses frames escolhidos foram: __frame0.jpg, frame161.jpg, frame260.jpg, frame702.jpg e frame800.jpg__. \n",
    "\n",
    "__OBS: Para que as imagens sejam salvas, a pasta ./frames precisa ter sido criada previamente.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5645462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrai os frames do vídeo para realizar a calibração da câmera\n",
    "\n",
    "cam = cv2.VideoCapture(\"./entrada.mp4\")\n",
    "\n",
    "current_frame = 0\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    if ret:\n",
    "        # Para esse trecho funcionar uma pasta ./frames deve existir no diretório do notebook\n",
    "        name = './frames/frame' + str(current_frame) + '.jpg'\n",
    "  \n",
    "        cv2.imwrite(name, frame)\n",
    "  \n",
    "        current_frame += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5825d",
   "metadata": {},
   "source": [
    "## Obtendo os parâmetros intrínsecos e os coeficientes de distorção\n",
    "\n",
    "Com os frames selecionados, foi usado o __MatLAB__ para obter os parâmetros intrísecos da câmera. A opção \"Camera Calibration\" foi a escolhida. Os pontos do tabuleiro foram selecionados e, após o mapeamento e as devidas funções internas do MatLAB terem sido executadas, a matriz de parâmetros intrínsecos foi obtida. Além disso, a partir dessa calibração os coeficientes de distorção também puderam ser obtidos. O MatLAB foi escolhido apenas por praticidade, afinal, softwares como o __Octave__ também seriam completamente adequados para realizar essa tarefa (a função __cv2::calibrateCamera()__ da OpenCV também poderia ser utilizada mas, por orientação do professor, a calibração foi realizada extra-código, no MatLAB).\n",
    "\n",
    "A próxima célula apresenta a declaração dessas matrizes com os dados de output do MatLAB.\n",
    "\n",
    "__OBS: A matriz informada pelo MatLAB é transposta da matriz utilizada nesse Notebook. Ela foi transposta para se parecer mais com a matriz apresentada durante as aulas e poder ser utilizada corretamente pelas funções da OpenCV e OpenGL.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5e30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[410.5564    0.      313.4223 ]\n",
      " [  0.      409.9881  233.66579]\n",
      " [  0.        0.        1.     ]]\n",
      "[ 0.09497973 -0.25836942  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Matriz com os parâmetros intrínsecos da câmera\n",
    "intrinsic_params = np.array(([410.556385620329, 0, 313.422287942592],\n",
    "                             [0, 409.988094875811, 233.665786823660],\n",
    "                             [0, 0, 1]), dtype=\"float32\")\n",
    "\n",
    "# Vetor com os coeficientes de distorção obtidos na calibração\n",
    "distortion_coefficients = np.array([0.094979724334344, -0.258369409395469, 0, 0], dtype=\"float32\")\n",
    "\n",
    "print(intrinsic_params)\n",
    "print(distortion_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b321054",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Localizando os alvos ao longo do vídeo\n",
    "\n",
    "A célula a seguir apresenta funções que são utilizadas na identificação dos alvos ao longo do frame. Estão descritas algumas funções intermerdiárias/auxiliares, como a __normalized_cross_correlation()__ e a __compare_target()__; juntamente com a função principal, que retorna o frame no qual os alvos identificados estão contornados de azul claro, as coordenadas de todos os alvos no frame e a orientação de cada um desses alvos. A seguir, cada função será melhor explicada/especificada: \n",
    "\n",
    "#### __normalized_cross_correlation():__\n",
    "Recebe um possível alvo retirado de um frame do vídeo e o compara com o template original do alvo pelo método da Correlação Cruzada Normalizada.\n",
    "\n",
    "O resultado é um valor em ponto flutuante entre 0 e 1 que indica a relação entre ambas as imagens: 1 elas são muito parecidas e 0 elas não são nada parecidas. Qualquer função que mede similaridade entre imagens poderia ser usada, como o RMSE ou MAE. Portanto, a escolha da função foi completamente arbitrária, visto que todas essas funções foram discutidas durante as aulas e estariam aptas a serem usadas.\n",
    "\n",
    "#### __find_orientation():__\n",
    "Recebe o possível alvo retificado e o template original do alvo.\n",
    "\n",
    "Com isso, a função rotaciona o alvo em todas as direções possíveis e calcula a __Correlação Cruzada Normalizada__, através da função __normalized_cross_correlation()__, para cada uma das direções e escolhe aquela que resulte no maior coeficiente (mais semelhante). Se esse valor estiver entre __0.65 e 1__, limiares definidos de maneira arbitrária, a partir de alguns testes com o vídeo; a função retorna __True__ para alvo e retorna qual a orientação desse alvo: __0 -> aponta para Cima__, __1 -> aponta para a Direita__, __2 -> aponta para Baixo__ e __3 -> aponta para a Esquerda__. Caso não seja um alvo, a função retorna __False__ e ã direção __-1__, indicando que esse polígono não se encaixa como um alvo na imagem.\n",
    "\n",
    "#### __compare_target():__\n",
    "Recebe como parâmetros o frame binarizado, as 4 extremidades do possível alvo e o frame original.\n",
    "\n",
    "Essa função é responsável por calcular a homografia, através da função __cv2::findHomography()__, do possível alvo, passando esse objeto para o mesmo plano do template original do alvo, que é carregado do diretório por essa função. Após isso, uma transformação perspectiva é feita, com a função __cv2::warpPerspective()__, usando como base a matriz de homografia calculada no passo anterior, e, com isso, a imagem do possível alvo estará retificada e poderá ser comparada com o template original. Após isso, a função __find_orientation()__ será chamada, passando o possível alvo retificado e o template, e seu resultado é retornado pela função __compare_target()__.\n",
    "\n",
    "Seria possível fazer essa reitificação a partir de outros métodos ou usar outras funções da OpenCV, mas como a matriz de homografia foi uma das formas vistas em sala de aula e seu funcionamento foi bem discutido, achei conveniente retificar o conjunto de pontos a partir dela.\n",
    "\n",
    "#### __identify_targets():__\n",
    "É a função principal da etapa de detecção dos alvos no frame. Só recebe o frame original do vídeo como entrada e processa todas as informações necessárias.\n",
    "\n",
    "A abordagem usada para identificar os alvos será a partir da identificação das bordas no frame. Dessa forma, com as bordas enfatizadas, a função que identifica contornos age de maneira mais efetiva. Depois disso, basta filtrar bem os contornos e testá-los, para comparar se são alvos ou não. Os que derem match com o template original serão enviados para um vetor separado, junto com sua orientação, e, ao final, a função retorna esses marcadores e o frame original do vídeo com os alvos contornados de azul claro.\n",
    "\n",
    "Para tanto, primeiramente, o frame é convertido para tons de cinza e é binarizado pela função __cv2::threshold()__. Ele será binarizado pois facilita a identificação de suas bordas e, com isso, a identificação dos contornos dos alvos. A função __cv2::findContours()__, então, é usada para identificar os contornos a partir das bordas enfatizadas. Para filtrar um pouco quais contornos obtidos serão analisados, o formato de suas arestas é aproximado, pela função __cv2::approxPolyDPapproxPolyDP()__, e o perímetro de cada polígono obtido é calculado, pela função __cv2::arcLength()__. Caso o polígono tenha 4 vértices e perímetro entre 140 e 450, significa que ele potencialmente é um alvo no frame. Então, esse possível alvo vai ser enviado para as funções de comparação e, caso ele realmente seja um alvo, ele é inserido no vetor __target_contours__, que contém os contornos a serem traçados na imagem, e tem o formato de seu array manipulado para ser usado pela função de renderização 3D, sendo inserido no vetor __targets__ após a formatação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe8814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a Correlação Cruzada Normalizada entre um possível alvo na cena e o template original a ser identificado\n",
    "def normalized_cross_correlation(target, template):\n",
    "    np.seterr(invalid='ignore')\n",
    "    \n",
    "    target_normalized = (target - target.mean()) / np.std(target)\n",
    "    template_normalized = (template - template.mean()) / np.std(template)\n",
    "    \n",
    "    return np.mean(target_normalized * template_normalized)\n",
    "\n",
    "# Dado o possível alvo retificado, calcula sua semelhança com todas as possíveis rotações do\n",
    "# template e retorna, primeiramente, se o possível alvo é de fato um alvo e qual a sua orientação, caso seja.\n",
    "def find_orientation(rectified, template):\n",
    "    # Rotaciona o template nas direções possíveis\n",
    "    up = template.copy()\n",
    "    right = cv2.rotate(template, cv2.ROTATE_90_CLOCKWISE)\n",
    "    down = cv2.rotate(template, cv2.ROTATE_180)\n",
    "    left = cv2.rotate(template, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        \n",
    "    # Calcula a similaridade para cada uma delas\n",
    "    similarity_up = normalized_cross_correlation(rectified, up)\n",
    "    similarity_right = normalized_cross_correlation(rectified, right)    \n",
    "    similarity_down = normalized_cross_correlation(rectified, down)\n",
    "    similarity_left = normalized_cross_correlation(rectified, left)\n",
    "    \n",
    "    similarities = [similarity_up, similarity_right, similarity_down, similarity_left]\n",
    "    \n",
    "    # Identifica a maior similaridade obtida\n",
    "    max_similarity = max(similarities)\n",
    "    max_index = similarities.index(max_similarity)\n",
    "    \n",
    "    # Caso a similaridade esteja no limiar estabelecido, o alvo foi identificado e terá sua orientação retornada\n",
    "    # 0 -> cima, 1 -> direita, 2 -> baixo, 3 -> esquerda\n",
    "    if max_similarity > 0.65 and max_similarity < 1:\n",
    "        return True, max_index\n",
    "    else:\n",
    "        return False, -1\n",
    "    \n",
    "# Dado o frame com um possível alvo e as coordenadas do alvo, retifica a imagem para saber se é um alvo ou não \n",
    "def compare_target(thresh, target, copy):\n",
    "    template = cv2.imread(\"./alvo.jpg\", 0)\n",
    "    \n",
    "    # Cria a matriz com os pontos do template\n",
    "    template_points = np.array([[0, 0],\n",
    "                            [template.shape[0], 0],\n",
    "                            [template.shape[0], template.shape[1]],\n",
    "                            [0, template.shape[1]]])\n",
    "    \n",
    "    # Calcula a homografia do possível alvo em relação a matriz dos pontos do template\n",
    "    homography, _ = cv2.findHomography(target, template_points)\n",
    "    \n",
    "    # Retifica o alvo com a matriz de homografia\n",
    "    rectified = cv2.warpPerspective(thresh, homography, (template.shape[0], template.shape[1]))\n",
    "        \n",
    "    # Obtém se ele é um alvo e qual a orientação dele, caso seja\n",
    "    is_target, orientation = find_orientation(rectified, template)\n",
    "    \n",
    "    return is_target, orientation\n",
    "    \n",
    "# Função principal que usa todas as outras para identificar os alvos em cada frame\n",
    "def identify_targets(frame):\n",
    "    copy = frame.copy()\n",
    "    # Converte o frame para escala de tons de cinza\n",
    "    gray_frame = cv2.cvtColor(copy, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    # Calcula a binarização do frame\n",
    "    _, thresh = cv2.threshold(gray_frame, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "    # Enfatiza as bordas presentes no frame binarizado\n",
    "    edged = cv2.Canny(thresh, 80, 160)\n",
    "    \n",
    "    # Cria os contornos ao longo das bordas do frame\n",
    "    contours, _ = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "    target_contours = []\n",
    "    targets = []\n",
    "\n",
    "    # Para cada contorno identificado\n",
    "    for contour in contours:\n",
    "        # Aproxima sua forma geométrica para obter o número de lados\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, perimeter * 0.1, True)\n",
    "        no_vertices = len(approx)\n",
    "        \n",
    "        # Caso o contorno identificado tenha 4 lados e seja convexo\n",
    "        if no_vertices == 4 and perimeter >= 140 and perimeter <= 450:\n",
    "            # Verifica se ele é um alvo\n",
    "            is_target, orientation = compare_target(thresh, approx, copy)\n",
    "                        \n",
    "            # Caso seja um alvo, insere no vetor de alvos identificados naquele frame\n",
    "            if is_target:\n",
    "                target_contours.append(approx)\n",
    "                targets.append((approx[:, 0][:].astype(\"float32\"), orientation))\n",
    "                \n",
    "    # Desenha de azul os contornos identificados no frame\n",
    "    cv2.drawContours(copy, target_contours, -1, (255, 255, 0), 3)\n",
    "    \n",
    "    # Retorna o frame contornado e os alvos identificados nesse frame\n",
    "    return copy, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee94c79-9cf6-4933-9a28-929a0f3f311a",
   "metadata": {},
   "source": [
    "## Funções da OpenGL\n",
    "\n",
    "A célula a seguir contém a implementação das principais funções usadas pela OpenGL. Estão definidas desde funções auxiliares executadas pelo código até as funções principais da OpenGL, que definem callbacks e outros aspectos importantes para a configuração correta da máquina de estados da OpenGL.\n",
    "\n",
    "#### __initOpenGL():__\n",
    "Recebe os parâmetros da câmera e as diensões da janela e inicializa o cone de perspectiva da câmera principal do OpenGL.\n",
    "\n",
    "#### __load_background():__\n",
    "Recebe como parâmetro o frame original do vídeo.\n",
    "\n",
    "Essa função vai gerar o ID para a textura do background e manipular o frame para que ele possa ser usado pela OpenGL como textura de um quadrado que vai representar o fundo do vídeo. Para isso, ela converte o sistema de cores da imagem, que é carregado pela OpenCV em BGR, para RGB usando a função __cv2::cvtColor()__, faz o flip na imagem no eixo X através da função __cv2::flip()__, para alinhar os eixos de coordenadas da OpenCV com a OpenGL. Depois disso, ela transforma a imagem texto, para que ela possa ser utilizada como textura pela OpenGL e define os filtros de escala da imagem, no caso, aproximação a partir dos vizinhos mais próximos. Por fim, a função retorna o ID gerado para o background e o background que foi transformado.\n",
    "\n",
    "#### __place_background():__\n",
    "É a função responsável por pegar o ID do background e o background propriamente na forma de string para inserí-lo como textura de fundo da janela. Para tal, um quadrado com projeção ortogonal e com as dimensões da janela é definido e a textura do background, que foi previamente processada, é vinculada a ele. Dessa forma, a imagem do frame aparece de fundo na janela.\n",
    "\n",
    "#### __verify_upper_target():__\n",
    "É a função responsável por identificar se o alvo passado como parâmetro é o alvo que aparece na parte de cima do vídeo. Essa verificação específica é feita para que seja possível fazer o alvo que fica mais acima girar no sentido horário, o contrário dos outros Pikachu's, que devem girar em sentido anti-horário.\n",
    "\n",
    "Em suma, dois limiares para as coordenadas X e Y foram definidos. Esses limiares foram escolhidos de maneira empírica, a partir de testes, até que os valores se adequassem as configurações do vídeo. Caso alguma coordenada do alvo analisado esteja dentro desses limiares, significa que esse alvo provavelmente é o alvo de cima, então o Pikachu em cima desse alvo rotaciona para o sentido contrário ao dos outros.\n",
    "\n",
    "#### __render_cube():__\n",
    "Recebe o tamanho do cubo a ser desenhado e utiliza a função __glutWireCube()__ para desenhar um cubo com o tamanho desejado na posição atual da câmera virtual da OpenGL. Essa função vai ser chamada logo após o cálculo da matriz de projeção para inserir o Pikachu no vídeo, assim, o cubo será desenhado no mesmo lugar que o Pikachu.\n",
    "\n",
    "#### __render_arrow():__\n",
    "Recebe o tamanho da flecha a ser desenhada e a orientação do alvo em que ela estará em cima.\n",
    "\n",
    "Dado esses parâmetros, o código estima para quais direções a linha e o cone serão transladados e qual será a rotação do cone para que a flecha seja devidamente montada na cena. A linha é criada com a função padrão da OpenGL para criar linhas e o cone é criado com a função __glutWireCone()__, da Glut. Assim como o cubo, a flecha será criada logo após a matriz de projeção que insere o Pikachu em cima do alvo ser criada, então ela será criada no centro do alvo e vai apontar para a direção que esse alvo está na cena.\n",
    "\n",
    "#### __render_3D_objects():__\n",
    "É a função principal da renderização 3D dos objetos. Recebe o objeto do Pikachu que foi carregado na função __displayCallback()__, os pontos do alvo e a orientação do alvo, os parâmetros intrínsecos da câmera e os coeficientes de distorção da câmera.\n",
    "\n",
    "Os pontos no mundo vão ser criados e, com eles, será possível calcular os vetores de rotação e translação, através da função __cv2::solvePnP()__. Os vetores de rotação serão transformados na matriz de rotação a partir da transformação de rotação de Rodrigues, que é calculada pela função __cv2::Rodrigues()__. Com a matriz de rotação e o vetor de translação, é possível montar a matriz de projeção para o alvo em questão. Alguns coeficientes dessa matriz serão negados para inverter os eixos Y e Z da OpenCV para OpenGL. A matriz de projeção é transposta e carregada na máquina de estados da OpenGL. Após isso, o Pikachu é renderizado em tela e as funções que renderizam o cubo e a flecha também são chamadas para que esses objetos apareçam na cena. O Pikachu, no entanto, será renderizado por último, pois antes dele aparecer em tela a função __glRotatef()__ aplica uma pequena rotação no eixo do objeto. Dessa forma, o cubo e a flecha não giram, mas o Pikachu é capaz de girar em torno de seu próprio eixo a medida que os frames do vídeo vão atualizando.\n",
    "\n",
    "#### __displayCallback():__\n",
    "Função principal para renderização dos frames e objetos 3D na janela da OpenGL.\n",
    "\n",
    "Carrega o objeto do Pikachu e busca um frame no vídeo. Se esse frame lido for válido (vídeo ainda não tiver sido finalizado), os alvos são identificados e o frame com os alvos contornados é passado para a função que o define como background da janela. Após isso, cada marcador é passado para a função que renderiza os objetos 3D na cena e o Pikachu, o cubo e a flecha são renderizados em cima de cada alvo. Com a renderização finalizada, a glut troca os buffers de imagem, com a função __glutSwapBuffers()__, e o próximo frame vai aparecer em tela.\n",
    "\n",
    "#### __idleCallback():__\n",
    "Chama a função da Glut que define que o frame atual precisa ser remontado, então a função de display é chamada novamente e um novo frame do vídeo vai ser colocado em tela. Além disso, o valor do ângulo de rotação do Pikachu é redefinido nesse callback. Assim, ele é capaz de rotacionar 6 graus a cada frame e, quando ultrapassar 360, volta para 0 e continua girando de maneira automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915ac2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza os ajustes de perspectiva da câmera do OpenGL\n",
    "def initOpenGL(intrinsic_params, dimensions):\n",
    "    (width, height) = dimensions\n",
    "    \n",
    "    # Cor do fundo ao realizar o clear\n",
    "    glClearColor(0.0, 0.0, 0.0, 0.0)\n",
    "    glClearDepth(1.0)\n",
    "\n",
    "    glEnable(GL_DEPTH_TEST)\n",
    "\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glLoadIdentity()\n",
    "    \n",
    "    fx = intrinsic_params[0, 0];\n",
    "    fy = intrinsic_params[1, 1];\n",
    "    \n",
    "    # Calcula o fovy e o aspect ratio e determina as distâncias 'near' e 'far' da câmera\n",
    "    fovy = 2 * np.arctan(0.5 * height/fy) * 180/np.pi;\n",
    "    aspect = (width * fy)/(height * fx);\n",
    "    near = 0.1;\n",
    "    far = 100.0;\n",
    "    \n",
    "    # Especifica o cone de visão da câmera\n",
    "    gluPerspective(fovy, aspect, near, far);\n",
    "\n",
    "# Gera o ID para a textura do background e manipula o background para ser usado como textura pela OpenGL\n",
    "def load_background(frame):\n",
    "    # Gera o ID para o background e o vincula a uma textura\n",
    "    background_id = glGenTextures(1)\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "    \n",
    "    # Converte o modo de cor OpenCV -> OpenGL e rotaciona os eixos\n",
    "    background = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    background = cv2.flip(background, 0)\n",
    "    \n",
    "    # Transforma a imagem para bytes, para ser usada como textura pela OpenGL\n",
    "    height, width, channels = background.shape\n",
    "    background = np.frombuffer(background.tobytes(), dtype=background.dtype, count = height * width * channels)    \n",
    "    background.shape = (height, width, channels)\n",
    "\n",
    "    # Cria a textura do background na OpenGL\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, background)\n",
    "    \n",
    "    # Retorna o ID do background e o background propriamente\n",
    "    return background_id, background\n",
    "\n",
    "# Cria um quadrado do tamanho da janela e projeta nele a textura do background\n",
    "def place_background(frame, dimensions):\n",
    "    # Pega o ID da textura e a textura do frame atual\n",
    "    background_id, background = load_background(frame)\n",
    "    \n",
    "    (width, height) = dimensions\n",
    "    \n",
    "    # Define uma projeção ortográfica com as dimensões iguais as da janela da OpenGL\n",
    "    glDepthMask(GL_FALSE)\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPushMatrix()\n",
    "    glLoadIdentity()\n",
    "    gluOrtho2D(0, width, 0, height)\n",
    "    \n",
    "    # Vincula o ID da textura ao background e o carrega como uma textura de texto\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, 3, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, background)\n",
    "    glPushMatrix()\n",
    "\n",
    "    # Cria os pontos de projeção do quadrado/background\n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2i(0, 0); glVertex2i(0, 0)\n",
    "    glTexCoord2i(1, 0); glVertex2i(width, 0)\n",
    "    glTexCoord2i(1, 1); glVertex2i(width, height)\n",
    "    glTexCoord2i(0, 1); glVertex2i(0, height)\n",
    "    glEnd()\n",
    "    \n",
    "    glPopMatrix()\n",
    "    \n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPopMatrix()\n",
    "    \n",
    "    # Volta o modo de matriz para MODELVIEW, disabilita a textura 2D e ativa a máscara de profundidade (\"reseta\" as configurações para o próximo frame)\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glDepthMask(GL_TRUE)\n",
    "    glDisable(GL_TEXTURE_2D)\n",
    "    glFlush()\n",
    "\n",
    "# Verifica se o alvo selecionado é o alvo mais acima presente no vídeo (alvo que gira no sentido horário)\n",
    "def verify_upper_target(target):\n",
    "    # Limiares setados a partir de testes executando o código\n",
    "    x_limit = 310\n",
    "    y_limit = 210\n",
    "    \n",
    "    # Se o alvo tem alguma coordenada que ultrapasse esse limiar, significa que é o alvo que gira no sentido horário\n",
    "    for coord in target:\n",
    "        if coord[0] >= x_limit and coord[1] <= y_limit:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "# Desenha o cubo na cena com auxilio da Glut\n",
    "def render_cube(length):\n",
    "    glutWireCube(length);\n",
    "\n",
    "# Desenha a flecha com a direção do alvo, tendo como centro o próprio alvo\n",
    "def render_arrow(length, orientation):\n",
    "    x_dir = 0\n",
    "    y_dir = 0\n",
    "\n",
    "    x_cone = 1\n",
    "    y_cone = 0\n",
    "    \n",
    "    # Série de IF's que verificam a orientação do alvo e configuram os parâmetros\n",
    "    # para translação e rotação da linha e cone que formam a flecha com direção do alvo\n",
    "    if orientation == 0:\n",
    "        x_dir = 0\n",
    "        y_dir = -length\n",
    "        x_cone = 1\n",
    "        y_cone = 0\n",
    "        \n",
    "    if orientation == 1:\n",
    "        x_dir = -length\n",
    "        y_dir = 0\n",
    "        x_cone = 0\n",
    "        y_cone = -1\n",
    "        \n",
    "    if orientation == 2:\n",
    "        x_dir = 0\n",
    "        y_dir = length\n",
    "        x_cone = -1\n",
    "        y_cone = 0\n",
    "        \n",
    "    if orientation == 3:\n",
    "        x_dir = length\n",
    "        y_dir = 0\n",
    "        x_cone = 0\n",
    "        y_cone = 1\n",
    "    \n",
    "    glPushMatrix()\n",
    "    \n",
    "    # Desenha a linha da flecha com a direção do alvo\n",
    "    glBegin(GL_LINES)\n",
    "    glVertex3d(0, 0, 0)\n",
    "    glVertex3d(x_dir, y_dir, 0)\n",
    "    glEnd()\n",
    "    \n",
    "    # Desenha e rotaciona a ponta da flecha com a direção do alvo\n",
    "    glTranslated(x_dir, y_dir, 0)\n",
    "    glRotated(90, x_cone, y_cone, 0)\n",
    "    glutWireCone(0.4, 1.4, 10, 10)\n",
    "    \n",
    "    glPopMatrix()\n",
    "    \n",
    "# Renderiza o Pikachu, o cubo e a flecha com a direção do alvo na cena\n",
    "def render_3D_objects(obj, target_points, orientation, intrinsic_params, distortion_coefficients):\n",
    "    # Pontos no mundo definidos de maneira arbitrária\n",
    "    object_points = np.array([[1, -1, 0], [-1, -1, 0],\n",
    "                              [-1, 1, 0], [1, 1, 0]], dtype=\"float32\")\n",
    "    \n",
    "    # Calcula os vetores de rotação e translação\n",
    "    _, rotation_vec, translation_vec = cv2.solvePnP(object_points, target_points, intrinsic_params, distortion_coefficients)\n",
    "     \n",
    "    # Converte o vetor de rotação para uma matriz de rotação \n",
    "    rotation_matrix = cv2.Rodrigues(rotation_vec)[0]\n",
    "    \n",
    "    # Monta a matriz de projeção\n",
    "    # Os coeficientes da segunda e terceira linha foram negados para inverter os eixos Y e Z, para passar\n",
    "    # do sistema de coordenadas da OpenCV para OpenGL\n",
    "    projection_matrix = np.array([[rotation_matrix[0, 0], rotation_matrix[0, 1], rotation_matrix[0, 2], translation_vec[0]],\n",
    "                                [-rotation_matrix[1, 0], -rotation_matrix[1, 1], -rotation_matrix[1, 2], -translation_vec[1]],\n",
    "                                [-rotation_matrix[2, 0], -rotation_matrix[2, 1], -rotation_matrix[2, 2], -translation_vec[2]],\n",
    "                                [0, 0, 0, 1]], dtype=\"float32\")\n",
    "    \n",
    "    # Transpõe a matriz\n",
    "    projection_matrix = projection_matrix.T\n",
    "    \n",
    "    glPushMatrix()\n",
    "        \n",
    "    # Carrega a matriz de projeção no OpenGL\n",
    "    glLoadMatrixf(projection_matrix)\n",
    "    \n",
    "    # Renderiza um cubo ao redor do alvo\n",
    "    render_cube(3)\n",
    "    \n",
    "    # Renderiza a flecha que aponta para a direção do alvo\n",
    "    render_arrow(3, orientation)\n",
    "    \n",
    "    is_upper_target = verify_upper_target(target_points)\n",
    "    \n",
    "    # Carrega a rotação do Pikachu para aquele frame\n",
    "    if is_upper_target:\n",
    "        glRotatef(-angle, 0, 0, 1)\n",
    "    else:\n",
    "        glRotatef(angle, 0, 0, 1)\n",
    "    \n",
    "    # Renderiza o modelo do Pikachu\n",
    "    glCallList(obj.gl_list)\n",
    "    \n",
    "    glPopMatrix()\n",
    "    \n",
    "def displayCallback():\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    \n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "    \n",
    "    # Carrega o objeto do Pikachu\n",
    "    obj = OBJ(\"Pikachu.obj\", swapyz=True)\n",
    "        \n",
    "    # Lê um frame do vídeo\n",
    "    ret, frame = video.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Identifica os alvos e pega o frame do vídeo com os alvos identificados contornados\n",
    "        frame_with_contour, targets = identify_targets(frame)\n",
    "                \n",
    "        # Define esse frame como background da janela do OpenGL\n",
    "        place_background(frame_with_contour, (640, 480))\n",
    "                \n",
    "        # Para cada target identificado\n",
    "        for target in targets:\n",
    "            target_coords, orientation = target\n",
    "            \n",
    "            # Renderiza os objetos 3D na cena (Pikachu, cubo e flecha direcionada)\n",
    "            render_3D_objects(obj, target_coords, orientation, intrinsic_params, distortion_coefficients)\n",
    "        \n",
    "        # Troca o buffer da janela atual\n",
    "        glutSwapBuffers()\n",
    "\n",
    "def idleCallback():\n",
    "    # Acessa a variável de rotação do Pikachu e altera seu valor\n",
    "    global angle\n",
    "    angle += 6\n",
    "    \n",
    "    # Caso ela passe dos 360 graus, retorna seu valor pra 0\n",
    "    if angle > 360:\n",
    "        angle = 0\n",
    "\n",
    "    glutPostRedisplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464b3d3-11f6-4896-b257-ef78c0070326",
   "metadata": {},
   "source": [
    "## Main da OpenGL\n",
    "\n",
    "Função principal do OpenGL. É a célula que cria a janela e executa as funções auxiliares e de callback definidas nas células anteriores. Essa célula vai carregar o vídeo de entrada do diretório, inicializar o OpenGL e a Glut e, por fim, criar uma janela com o título __\"TP2 - Realidade Aumentada - Vinicius Silva Gomes\"__ e executar o vídeo com os alvos contornados e os objetos 3D renderizados em cima de cada alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee205f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6343/37168942.py:169: DeprecationWarning: setting an array element with a sequence. This was supported in some cases where the elements are arrays with a single element. For example `np.array([1, np.array([2])], dtype=int)`. In the future this will raise the same ValueError as `np.array([1, [2]], dtype=int)`.\n",
      "  projection_matrix = np.array([[rotation_matrix[0, 0], rotation_matrix[0, 1], rotation_matrix[0, 2], translation_vec[0]],\n"
     ]
    }
   ],
   "source": [
    "dimensions = (640, 480)\n",
    "\n",
    "# Parâmetros intrínsecos e coeficientes de distorção obtidos com a calibração\n",
    "intrinsic_params = np.array(([410.556385620329, 0, 313.422287942592],\n",
    "                             [0, 409.988094875811, 233.665786823660],\n",
    "                             [0, 0, 1]), dtype=\"float32\")\n",
    "\n",
    "# Vetor com os coeficientes de distorção obtidos na calibração\n",
    "distortion_coefficients = np.array([0.094979724334344, -0.258369409395469, 0, 0], dtype=\"float32\")\n",
    "\n",
    "# Carrega o vídeo da pasta\n",
    "video = cv2.VideoCapture(\"./entrada.mp4\")\n",
    "\n",
    "# Variável que indica o ângulo de rotação do Pikachu\n",
    "angle = 0\n",
    "\n",
    "# Inicializa a glut\n",
    "glutInit()\n",
    "glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH)\n",
    "glutSetOption(GLUT_ACTION_ON_WINDOW_CLOSE, GLUT_ACTION_CONTINUE_EXECUTION)\n",
    "\n",
    "glutInitWindowSize(*dimensions)\n",
    "\n",
    "# Cria a janela e inicializa o OpenGL com os parâmetros da câmera e as dimensões da janela\n",
    "window = glutCreateWindow(b'TP2 - Realidade Aumentada - Vinicius Silva Gomes')\n",
    "initOpenGL(intrinsic_params, dimensions)\n",
    "\n",
    "# Chama as funções principais de display e entra no loop principal do OpenGL\n",
    "glutDisplayFunc(displayCallback)\n",
    "glutIdleFunc(idleCallback)\n",
    "\n",
    "# Chama o loop principal do OpenGL\n",
    "glutMainLoop()\n",
    "\n",
    "# Desaloca o vídeo de entrada\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57217bd1-b441-4979-b692-ee3cbffe2844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993d217-53c6-4fa1-b5bb-e5257c5549cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
